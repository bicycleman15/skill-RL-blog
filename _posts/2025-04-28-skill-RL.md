---
layout: distill  
title: "On skill, curriculum and shortcut learning in RL for reasoning" 
description: "What to do your base model has zero success rate on a task, and you want to RL?"
date: 2025-04-28  
future: true  
htmlwidgets: true  
hidden: false  

authors:  
  - name: Anirudh Buvanesh
    url: "https://anirudhb11.github.io/"
    affiliations:
      name: MILA
  - name: Jatin Prakash
    url: "https://bicycleman15.github.io/"
    affiliations:
      name: NYU

bibliography:  2025-04-28-skill-RL.bib

toc:  
  - name: Takeaways
  - name: What to do when you have zero rewards?
  - name: Setting baselines
    subsections:
    - name: What's the task?
    - name: Naive RL using sparse rewards fails
    - name: Reward shaping to obtain dense rewards donâ€™t really work
  - name: A simple intervention helps
    subsections:  
    - name: Not all easy samples work
    - name: Final method
    - name: Why does this work?
  - name: Conclusions and some related work
    subsections:
    - name: Future work
    - name: Limitations
  - name: Acknowledgements
  - name: Appendix  

---

## Takeaways

1. In sparse-reward settings, where a base model fails to solve a task even after prolonged RL training (i.e., rewards remain zero), a simple data-centric intervention of augmenting the dataset with easier instances enables transfer to harder problems and yields strong improvements.

2. The choice of data used for augmentation matters! Including very easy samples of the task results in shortcut learning and prevents the model from acquiring the intended skill

3. We compare the effect of our data-centric intervention in sparse-reward settings to approaches that incorporate dense rewards through reward shaping <d-cite key=""></d-cite> and find that dense rewards donâ€™t perform well in the settings we study. Since there was no official implementation of this baseline, weâ€™re releasing our code as wellâ€”hopefully youâ€™ll find it useful in your own experiments ðŸ™‚

4. We conclude with a practical recipe for RL practitioners: whenever possible, supplement training with easier instances of the same task. We also situate this finding in the context of skill learning and discuss related work with a similar flavor.

> We scope our experiments to a simple graph search problem introduced in <d-cite key=""></d-cite> and leave the exploration of other task types as future work. Nevertheless, we think the findings are interesting enough to share ðŸ™‚